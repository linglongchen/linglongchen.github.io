# 初识Transformer


## 什么是transformer





## 注意力机制


## token是什么？



## 主要流程


### 输入表示
分词：首先将输入的原始文本分割成较小的单位，通常是单词或子词单元，每个单词或者子单元都是一个token。
位置编码：由于Transformer不包含递归结构，因此需要显式地加入位置信息。为每个位置添加一个固定的位置编码，以便模型可以学习到序列中元素的相对位置。

### embedding
将输入的信息拆分为tokens，然后再将tokens转换为向量表示。


### Encoder
将输入的向量进行编码，得到编码器上下文

### Decoder



### 输出

